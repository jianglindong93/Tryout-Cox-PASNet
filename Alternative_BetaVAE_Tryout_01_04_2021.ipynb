{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from scipy.interpolate import interp1d\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data.sort_values(\"OS.time\",ascending = False, inplace = True)\n",
    "    x = data.drop([\"Patient_ID\", \"race_black\", \"race_white\", \"age\", \"stageh\",\"gradeh\", \"OS\", \"OS.time\"], axis = 1).values\n",
    "    ytime = data.loc[:, [\"OS.time\"]].values\n",
    "    yevent = data.loc[:, [\"OS\"]].values\n",
    "    age = data.loc[:, [\"age\"]].values\n",
    "    cstage = data.loc[:, [\"stageh\"]].values\n",
    "    hgrade = data.loc[:, [\"gradeh\"]].values\n",
    "    race_black = data.loc[:, [\"race_black\"]].values\n",
    "    race_white = data.loc[:, [\"race_white\"]].values\n",
    "    return(x, ytime, yevent, age, cstage, hgrade, race_black, race_white)\n",
    "\n",
    "def load_data(path, dtype):\n",
    "    x, ytime, yevent, age, cstage, hgrade, race_black, race_white = sort_data(path)\n",
    "    X = torch.from_numpy(x).type(dtype)\n",
    "    YTIME = torch.from_numpy(ytime).type(dtype)\n",
    "    YEVENT = torch.from_numpy(yevent).type(dtype)\n",
    "    AGE = torch.from_numpy(age).type(dtype)\n",
    "    CSTAGE = torch.from_numpy(cstage).type(dtype)\n",
    "    HGRADE = torch.from_numpy(hgrade).type(dtype)\n",
    "    RACE_BLACK = torch.from_numpy(race_black).type(dtype)\n",
    "    RACE_WHITE = torch.from_numpy(race_white).type(dtype)\n",
    "    if torch.cuda.is_available():\n",
    "        X = X.cuda()\n",
    "        YTIME = YTIME.cuda()\n",
    "        YEVENT = YEVENT.cuda()\n",
    "        AGE = AGE.cuda()\n",
    "        CSTAGE = CSTAGE.cuda()\n",
    "        HGRADE = HGRADE.cuda()\n",
    "        RACE_BLACK = RACE_BLACK.cuda()\n",
    "        RACE_WHITE = RACE_WHITE.cuda()\n",
    "    return(X, YTIME, YEVENT, AGE, CSTAGE, HGRADE, RACE_BLACK, RACE_WHITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience, verbose=False, delta=0):\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter % 200 == 0:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(x, x_recon):\n",
    "    batch_size = x.size(0)\n",
    "    assert batch_size != 0\n",
    "    \n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction='sum').div(batch_size)\n",
    "\n",
    "    return recon_loss\n",
    "\n",
    "def kl_divergence(mu, logvar):\n",
    "    batch_size = mu.size(0)\n",
    "    assert batch_size != 0\n",
    "    \n",
    "    klds = -0.5*(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    total_kld = klds.sum(1).mean(0, True)\n",
    "    dimension_wise_kld = klds.mean(0)\n",
    "    mean_kld = klds.mean(1).mean(0, True)\n",
    "\n",
    "    return total_kld, dimension_wise_kld, mean_kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparametrize(mu, logvar):\n",
    "    std = logvar.div(2).exp()\n",
    "    eps = Variable(std.data.new(std.size()).normal_())\n",
    "    return mu + std*eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaVAE_B(nn.Module):\n",
    "    \"\"\"Model proposed in understanding beta-VAE paper(Burgess et al, arxiv:1804.03599, 2018). Modifications made to best accommodate our data\"\"\"\n",
    "\n",
    "    def __init__(self, z_dim, input_n):\n",
    "        super(BetaVAE_B, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.nc = input_n\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_n, 200),          \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(200, 50),         \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(50, z_dim*2)            \n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, 50),                             \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(50, 200),      \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(200, input_n)\n",
    "        )\n",
    "        \n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                kaiming_init(m)\n",
    "\n",
    "    def forward(self, x):\n",
    "        distributions = self._encode(x)\n",
    "        mu = distributions[:, :self.z_dim]\n",
    "        logvar = distributions[:, self.z_dim:]\n",
    "        z = reparametrize(mu, logvar)\n",
    "        x_recon = self._decode(z)\n",
    "\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "    def _encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def _decode(self, z):\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBetaVAE_B(train_x, eval_x, z_dim, input_n, Learning_Rate, L2, Num_Epochs, patience, gamma, C_max, C_stop_iter):\n",
    "    net = BetaVAE_B(z_dim, input_n)\n",
    "    \n",
    "    #early_stopping = EarlyStopping(patience = patience, verbose = False)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    opt = optim.Adam(net.parameters(), lr=Learning_Rate, weight_decay = L2)\n",
    "    for epoch in range(Num_Epochs+1):\n",
    "        net.train()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        x_recon, mu, logvar = net(train_x)\n",
    "        recon_loss = reconstruction_loss(train_x, x_recon)\n",
    "        total_kld, dim_wise_kld, mean_kld = kl_divergence(mu, logvar)\n",
    "        C = torch.clamp(C_max/C_stop_iter*epoch, 0, C_max.data[0])\n",
    "        beta_vae_loss = recon_loss + gamma*(total_kld-C).abs()\n",
    "\n",
    "        \n",
    "        beta_vae_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        net.eval()\n",
    "        val_x_recon, val_mu, val_logvar = net(eval_x)\n",
    "        val_recon_loss = reconstruction_loss(eval_x, val_x_recon)\n",
    "        val_total_kld, val_dim_wise_kld, val_mean_kld = kl_divergence(val_mu, val_logvar)\n",
    "        val_loss = val_recon_loss + gamma*(val_total_kld-C).abs()\n",
    "        \n",
    "       # early_stopping(val_loss, net)\n",
    "       # if early_stopping.early_stop:\n",
    "       #     net.train()\n",
    "       #     tr_x_recon, tr_mu, tr_logvar = net(train_x)\n",
    "       #     tr_recon_loss = reconstruction_loss(train_x, tr_x_recon)\n",
    "       #     tr_total_kld, tr_dim_wise_kld, tr_mean_kld = kl_divergence(tr_mu, tr_logvar)\n",
    "       #     tr_loss = tr_recon_loss + gamma*(tr_total_kld-C).abs()\n",
    "       #     print(\"Early stopping, Number of epochs: \", epoch, \", Loss in Validation: \", val_loss, \", Loss in Training: \", tr_loss)\n",
    "       #     break\n",
    "        if epoch % 1000 == 0:\n",
    "            net.train()\n",
    "            tr_x_recon, tr_mu, tr_logvar = net(train_x)\n",
    "            tr_recon_loss = reconstruction_loss(train_x, tr_x_recon)\n",
    "            tr_total_kld, tr_dim_wise_kld, tr_mean_kld = kl_divergence(tr_mu, tr_logvar)\n",
    "            tr_loss = tr_recon_loss + gamma*(tr_total_kld-C).abs()\n",
    "            print(\"Number of epochs: \", epoch, \", Loss in Train: \", tr_loss)\n",
    "    return (tr_loss, val_loss, tr_mu, tr_logvar, val_mu, val_logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs:  0 , Loss in Train:  tensor([6292.1270], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  1000 , Loss in Train:  tensor([951.2902], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  2000 , Loss in Train:  tensor([935.1714], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  3000 , Loss in Train:  tensor([943.6773], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  4000 , Loss in Train:  tensor([921.4156], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  5000 , Loss in Train:  tensor([932.7629], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  6000 , Loss in Train:  tensor([930.8020], grad_fn=<AddBackward0>)\n",
      "L2:  0.1 , LR:  0.01 , Loss in Validation:  tensor([1006.1946], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  0 , Loss in Train:  tensor([10005.4824], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  1000 , Loss in Train:  tensor([936.8536], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  2000 , Loss in Train:  tensor([924.2093], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  3000 , Loss in Train:  tensor([935.7556], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  4000 , Loss in Train:  tensor([920.3140], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  5000 , Loss in Train:  tensor([933.3916], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  6000 , Loss in Train:  tensor([939.0139], grad_fn=<AddBackward0>)\n",
      "L2:  0.01 , LR:  0.01 , Loss in Validation:  tensor([1617.3821], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  0 , Loss in Train:  tensor([12720.3193], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  1000 , Loss in Train:  tensor([935.8242], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  2000 , Loss in Train:  tensor([930.2415], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  3000 , Loss in Train:  tensor([979.9073], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  4000 , Loss in Train:  tensor([942.0941], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  5000 , Loss in Train:  tensor([931.8400], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  6000 , Loss in Train:  tensor([959.5963], grad_fn=<AddBackward0>)\n",
      "L2:  0.005 , LR:  0.01 , Loss in Validation:  tensor([2738.6138], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  0 , Loss in Train:  tensor([9766.8848], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  1000 , Loss in Train:  tensor([966.6722], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  2000 , Loss in Train:  tensor([974.1431], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  3000 , Loss in Train:  tensor([972.9987], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  4000 , Loss in Train:  tensor([913.9835], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  5000 , Loss in Train:  tensor([909.5518], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  6000 , Loss in Train:  tensor([977.4348], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  7000 , Loss in Train:  tensor([914.5139], grad_fn=<AddBackward0>)\n",
      "Optimal L2:  0.1 , Optimal LR:  0.01\n"
     ]
    }
   ],
   "source": [
    "z_dim = 10\n",
    "input_n = 929\n",
    "Initial_Learning_Rate = [0.01]\n",
    "L2_Lambda = [0.1, 0.01, 0.005]\n",
    "patience = 1000\n",
    "gamma = 500\n",
    "C_max = torch.tensor([25.])\n",
    "C_stop_iter = 5000\n",
    "num_epochs = 6000\n",
    "Num_EPOCHS = 7000\n",
    "x_train, ytime_train, yevent_train, age_train, cstage_train, hgrade_train, race_black_train, race_white_train = load_data(\"D:/DL/Variational autoencoder/Tryout_12_30_2020/divided_data/exp_20/data_tr_20.csv\", dtype)\n",
    "x_valid, ytime_valid, yevent_valid, age_valid, cstage_valid, hgrade_valid, race_black_valid, race_white_valid = load_data(\"D:/DL/Variational autoencoder/Tryout_12_30_2020/divided_data/exp_20/data_val_20.csv\", dtype)\n",
    "x_test, ytime_test, yevent_test, age_test, cstage_test, hgrade_test, race_black_test, race_white_test = load_data(\"D:/DL/Variational autoencoder/Tryout_12_30_2020/divided_data/exp_20/data_tes_20.csv\", dtype)\n",
    "opt_l2_loss = 0\n",
    "opt_lr_loss = 0\n",
    "opt_loss = torch.Tensor([float(\"Inf\")])\n",
    "if torch.cuda.is_available():\n",
    "    opt_loss = opt_loss.cuda()\n",
    "for l2 in L2_Lambda:\n",
    "    for lr in Initial_Learning_Rate:\n",
    "        loss_train, loss_valid, tr_mu, tr_logvar, val_mu, val_logvar = trainBetaVAE_B(x_train, x_valid, z_dim, input_n, lr, l2, num_epochs, patience, gamma, C_max, C_stop_iter)\n",
    "        if loss_valid < opt_loss:\n",
    "            opt_l2_loss = l2\n",
    "            opt_lr_loss = lr\n",
    "            opt_loss = loss_valid\n",
    "        print (\"L2: \", l2, \", LR: \", lr, \", Loss in Validation: \", loss_valid)\n",
    "loss_train, loss_test, tr_mu, tr_logvar, tes_mu, tes_logvar = trainBetaVAE_B(x_train, x_test, z_dim, input_n, opt_lr_loss, opt_l2_loss, Num_EPOCHS, patience, gamma, C_max, C_stop_iter)\n",
    "print (\"Optimal L2: \", opt_l2_loss, \", Optimal LR: \", opt_lr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([270, 10])\n"
     ]
    }
   ],
   "source": [
    "tr_z = reparametrize(tr_mu, tr_logvar)\n",
    "tes_z = reparametrize(tes_mu, tes_logvar)\n",
    "\n",
    "print(tr_z.size())\n",
    "\n",
    "processed_tr_pre = torch.cat((tr_z, ytime_train, yevent_train, age_train, cstage_train, hgrade_train, race_black_train, race_white_train), 1)\n",
    "processed_tes_pre = torch.cat((tes_z, ytime_test, yevent_test, age_test, cstage_test, hgrade_test, race_black_test, race_white_test), 1)\n",
    "\n",
    "processed_tr = pd.DataFrame(processed_tr_pre, columns = ['Z_1', 'Z_2', 'Z_3', 'Z_4', 'Z_5', 'Z_6', 'Z_7', \n",
    "                                                         'Z_8', 'Z_9', 'Z_10', 'OS.time', 'OS.event', 'age', \n",
    "                                                         'stageh', 'gradeh', 'race_black', 'race_white'])\n",
    "processed_tr = processed_tr.astype(float)\n",
    "processed_tes = pd.DataFrame(processed_tes_pre, columns = ['Z_1', 'Z_2', 'Z_3', 'Z_4', 'Z_5', 'Z_6', 'Z_7', \n",
    "                                                           'Z_8', 'Z_9', 'Z_10', 'OS.time', 'OS.event', 'age', \n",
    "                                                           'stageh', 'gradeh', 'race_black', 'race_white'])\n",
    "processed_tes = processed_tes.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Z_1       Z_2       Z_3       Z_4       Z_5       Z_6       Z_7  \\\n",
      "0   1.316117  1.813122 -1.568688 -1.869874 -1.236799 -0.998041 -2.408681   \n",
      "1   1.362314  1.526981 -1.432127 -1.903233  3.350794 -2.169665 -1.809779   \n",
      "2   2.998492  1.540541 -1.896181 -2.402537  4.602023 -1.782509 -1.474900   \n",
      "3   2.163852  1.411169 -2.118542 -2.560777 -1.219030 -2.511004 -2.890308   \n",
      "4   1.218537  1.952838 -1.455242 -1.277790  1.768396 -2.052961 -2.911281   \n",
      "..       ...       ...       ...       ...       ...       ...       ...   \n",
      "73  1.867604  1.369310 -1.977803 -2.306443  2.900714 -1.236399 -2.946648   \n",
      "74  1.780923  1.613906 -1.808143 -1.868042 -0.599716 -1.951150 -2.428445   \n",
      "75  1.726435  1.605304 -1.928070 -2.128355  4.077435 -1.754271 -2.928323   \n",
      "76  2.099422  2.042235 -1.984882 -1.611757  2.922384 -2.261796 -2.214767   \n",
      "77  1.227224  1.895802 -1.668252 -2.126025  0.620667 -1.213066 -2.171818   \n",
      "\n",
      "         Z_8       Z_9      Z_10  OS.time  OS.event   age  stageh  gradeh  \\\n",
      "0   0.933801 -2.099832  1.684523   5481.0       0.0  59.0     1.0     1.0   \n",
      "1   1.223148 -1.624493  2.212202   4624.0       1.0  67.0     1.0     1.0   \n",
      "2   1.105701 -2.475116  2.603409   3819.0       1.0  66.0     1.0     1.0   \n",
      "3   1.325092 -1.338204  2.107424   3500.0       0.0  56.0     1.0     1.0   \n",
      "4   1.378410 -2.410960  2.085908   2661.0       0.0  66.0     1.0     1.0   \n",
      "..       ...       ...       ...      ...       ...   ...     ...     ...   \n",
      "73  1.325099 -2.041487  2.216117    138.0       1.0  63.0     1.0     1.0   \n",
      "74  1.316344 -2.019608  1.648148    129.0       1.0  65.0     1.0     1.0   \n",
      "75  1.303999 -2.479515  1.632550     61.0       1.0  66.0     1.0     1.0   \n",
      "76  1.569788 -1.774246  1.951358     44.0       0.0  52.0     1.0     1.0   \n",
      "77  1.029746 -2.410856  1.767563      8.0       1.0  64.0     1.0     1.0   \n",
      "\n",
      "    race_black  race_white  \n",
      "0          0.0         1.0  \n",
      "1          0.0         1.0  \n",
      "2          0.0         1.0  \n",
      "3          0.0         1.0  \n",
      "4          0.0         1.0  \n",
      "..         ...         ...  \n",
      "73         0.0         1.0  \n",
      "74         0.0         1.0  \n",
      "75         0.0         1.0  \n",
      "76         0.0         1.0  \n",
      "77         0.0         1.0  \n",
      "\n",
      "[78 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "print(processed_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lifelines\n",
    "from lifelines import CoxPHFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>lifelines.CoxPHFitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration col</th>\n",
       "      <td>'OS.time'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event col</th>\n",
       "      <td>'OS.event'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penalizer</th>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l1 ratio</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline estimation</th>\n",
       "      <td>breslow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of observations</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of events observed</th>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partial log-likelihood</th>\n",
       "      <td>-153.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time fit was run</th>\n",
       "      <td>2021-01-07 21:21:40 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>exp(coef)</th>\n",
       "      <th>se(coef)</th>\n",
       "      <th>coef lower 95%</th>\n",
       "      <th>coef upper 95%</th>\n",
       "      <th>exp(coef) lower 95%</th>\n",
       "      <th>exp(coef) upper 95%</th>\n",
       "      <th>z</th>\n",
       "      <th>p</th>\n",
       "      <th>-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Z_1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2.66</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_2</th>\n",
       "      <td>0.63</td>\n",
       "      <td>1.87</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.58</td>\n",
       "      <td>6.10</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_3</th>\n",
       "      <td>-0.55</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.52</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_4</th>\n",
       "      <td>0.66</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.65</td>\n",
       "      <td>5.76</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_5</th>\n",
       "      <td>0.22</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_6</th>\n",
       "      <td>0.71</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.78</td>\n",
       "      <td>5.29</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_7</th>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.96</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_8</th>\n",
       "      <td>0.37</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.56</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_9</th>\n",
       "      <td>0.42</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.56</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_10</th>\n",
       "      <td>-0.52</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-1.33</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.33</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stageh</th>\n",
       "      <td>1.42</td>\n",
       "      <td>4.12</td>\n",
       "      <td>1.07</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>33.25</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradeh</th>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-1.40</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.55</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_black</th>\n",
       "      <td>-10.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.87</td>\n",
       "      <td>-88.16</td>\n",
       "      <td>68.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.89e+29</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_white</th>\n",
       "      <td>-2.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.36</td>\n",
       "      <td>-4.81</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.69</td>\n",
       "      <td>-1.58</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Concordance</th>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Partial AIC</th>\n",
       "      <td>337.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-likelihood ratio test</th>\n",
       "      <td>23.70 on 15 df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-log2(p) of ll-ratio test</th>\n",
       "      <td>3.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cph = CoxPHFitter(l1_ratio = 1., penalizer = 0.0001)\n",
    "cph.fit(processed_tes, duration_col='OS.time', event_col='OS.event')\n",
    "cph.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
