{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from scipy.interpolate import interp1d\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data.sort_values(\"OS.time\",ascending = False, inplace = True)\n",
    "    x = data.drop([\"Patient_ID\", \"race_black\", \"race_white\", \"age\", \"stageh\",\"gradeh\", \"OS\", \"OS.time\"], axis = 1).values\n",
    "    ytime = data.loc[:, [\"OS.time\"]].values\n",
    "    yevent = data.loc[:, [\"OS\"]].values\n",
    "    age = data.loc[:, [\"age\"]].values\n",
    "    cstage = data.loc[:, [\"stageh\"]].values\n",
    "    hgrade = data.loc[:, [\"gradeh\"]].values\n",
    "    race_black = data.loc[:, [\"race_black\"]].values\n",
    "    race_white = data.loc[:, [\"race_white\"]].values\n",
    "    return(x, ytime, yevent, age, cstage, hgrade, race_black, race_white)\n",
    "\n",
    "def load_data(path, dtype):\n",
    "    x, ytime, yevent, age, cstage, hgrade, race_black, race_white = sort_data(path)\n",
    "    X = torch.from_numpy(x).type(dtype)\n",
    "    YTIME = torch.from_numpy(ytime).type(dtype)\n",
    "    YEVENT = torch.from_numpy(yevent).type(dtype)\n",
    "    AGE = torch.from_numpy(age).type(dtype)\n",
    "    CSTAGE = torch.from_numpy(cstage).type(dtype)\n",
    "    HGRADE = torch.from_numpy(hgrade).type(dtype)\n",
    "    RACE_BLACK = torch.from_numpy(race_black).type(dtype)\n",
    "    RACE_WHITE = torch.from_numpy(race_white).type(dtype)\n",
    "    if torch.cuda.is_available():\n",
    "        X = X.cuda()\n",
    "        YTIME = YTIME.cuda()\n",
    "        YEVENT = YEVENT.cuda()\n",
    "        AGE = AGE.cuda()\n",
    "        CSTAGE = CSTAGE.cuda()\n",
    "        HGRADE = HGRADE.cuda()\n",
    "        RACE_BLACK = RACE_BLACK.cuda()\n",
    "        RACE_WHITE = RACE_WHITE.cuda()\n",
    "    return(X, YTIME, YEVENT, AGE, CSTAGE, HGRADE, RACE_BLACK, RACE_WHITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience, verbose=False, delta=0):\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter % 20 == 0:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(x, x_recon):\n",
    "    batch_size = x.size(0)\n",
    "    assert batch_size != 0\n",
    "    \n",
    "    recon_loss = F.mse_loss(x_recon, x, size_average=False).div(batch_size)\n",
    "\n",
    "    return recon_loss\n",
    "\n",
    "def kl_divergence(mu, logvar):\n",
    "    batch_size = mu.size(0)\n",
    "    assert batch_size != 0\n",
    "    \n",
    "    klds = -0.5*(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    total_kld = klds.sum(1).mean(0, True)\n",
    "    dimension_wise_kld = klds.mean(0)\n",
    "    mean_kld = klds.mean(1).mean(0, True)\n",
    "\n",
    "    return total_kld, dimension_wise_kld, mean_kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparametrize(mu, logvar):\n",
    "    std = logvar.div(2).exp()\n",
    "    eps = Variable(std.data.new(std.size()).normal_())\n",
    "    return mu + std*eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaVAE_H(nn.Module):\n",
    "    \"\"\"Model proposed in original beta-VAE paper(Higgins et al, ICLR, 2017). Modifications made to best accommodate our data\"\"\"\n",
    "\n",
    "    def __init__(self, z_dim, input_n):\n",
    "        super(BetaVAE_H, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.nc = input_n\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_n, 200),          \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(200, 50),         \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(50, z_dim*2)            \n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, 50),                             \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(50, 200),      \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(200, input_n)\n",
    "        )\n",
    "        \n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                kaiming_init(m)\n",
    "\n",
    "    def forward(self, x):\n",
    "        distributions = self._encode(x)\n",
    "        mu = distributions[:, :self.z_dim]\n",
    "        logvar = distributions[:, self.z_dim:]\n",
    "        z = reparametrize(mu, logvar)\n",
    "        x_recon = self._decode(z)\n",
    "\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "    def _encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def _decode(self, z):\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBetaVAE_H(train_x, eval_x, z_dim, input_n, Learning_Rate, L2, Num_Epochs, patience, beta):\n",
    "    net = BetaVAE_H(z_dim, input_n)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience = patience, verbose = False)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    opt = optim.Adam(net.parameters(), lr=Learning_Rate, weight_decay = L2)\n",
    "    for epoch in range(Num_Epochs+1):\n",
    "        net.train()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        x_recon, mu, logvar = net(train_x)\n",
    "        recon_loss = reconstruction_loss(train_x, x_recon)\n",
    "        total_kld, dim_wise_kld, mean_kld = kl_divergence(mu, logvar)\n",
    "        beta_vae_loss = recon_loss + beta*total_kld\n",
    "        \n",
    "        beta_vae_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        net.eval()\n",
    "        val_x_recon, val_mu, val_logvar = net(eval_x)\n",
    "        val_recon_loss = reconstruction_loss(eval_x, val_x_recon)\n",
    "        val_total_kld, val_dim_wise_kld, val_mean_kld = kl_divergence(val_mu, val_logvar)\n",
    "        val_loss = val_recon_loss + beta*val_total_kld\n",
    "        \n",
    "        early_stopping(val_loss, net)\n",
    "        if early_stopping.early_stop:\n",
    "            net.train()\n",
    "            tr_x_recon, tr_mu, tr_logvar = net(train_x)\n",
    "            tr_recon_loss = reconstruction_loss(train_x, tr_x_recon)\n",
    "            tr_total_kld, tr_dim_wise_kld, tr_mean_kld = kl_divergence(tr_mu, tr_logvar)\n",
    "            tr_loss = tr_recon_loss + beta*tr_total_kld\n",
    "            print(\"Early stopping, Number of epochs: \", epoch, \", Loss in Validation: \", val_loss, \", Loss in Training: \", tr_loss)\n",
    "            break\n",
    "        if epoch % 200 == 0:\n",
    "            net.train()\n",
    "            tr_x_recon, tr_mu, tr_logvar = net(train_x)\n",
    "            tr_recon_loss = reconstruction_loss(train_x, tr_x_recon)\n",
    "            tr_total_kld, tr_dim_wise_kld, tr_mean_kld = kl_divergence(tr_mu, tr_logvar)\n",
    "            tr_loss = tr_recon_loss + beta*tr_total_kld\n",
    "            print(\"Loss in Train: \", tr_loss)\n",
    "    return (tr_loss, val_loss, tr_mu, tr_logvar, val_mu, val_logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in Train:  tensor([1.5432e+20], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "L2:  0.1 , LR:  0.03 , Loss in Validation:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([608042.], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Loss in Train:  tensor([927.0010], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "Loss in Train:  tensor([926.1686], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Early stopping, Number of epochs:  507 , Loss in Validation:  tensor([973.6490], grad_fn=<AddBackward0>) , Loss in Training:  tensor([926.2075], grad_fn=<AddBackward0>)\n",
      "L2:  0.1 , LR:  0.01 , Loss in Validation:  tensor([973.6490], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([26845.5078], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([992.2949], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([951.9249], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([935.8709], grad_fn=<AddBackward0>)\n",
      "L2:  0.1 , LR:  0.001 , Loss in Validation:  tensor([2821.5142], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([13718.9746], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([998.8839], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([951.0275], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([938.4326], grad_fn=<AddBackward0>)\n",
      "L2:  0.1 , LR:  0.00075 , Loss in Validation:  tensor([1915.8420], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([89364472.], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Early stopping, Number of epochs:  121 , Loss in Validation:  tensor([1018999.6250], grad_fn=<AddBackward0>) , Loss in Training:  tensor([2.9172e+23], grad_fn=<AddBackward0>)\n",
      "L2:  0.01 , LR:  0.03 , Loss in Validation:  tensor([1018999.6250], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([46527.5859], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([926.2697], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Loss in Train:  tensor([925.5925], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Early stopping, Number of epochs:  507 , Loss in Validation:  tensor([971.9980], grad_fn=<AddBackward0>) , Loss in Training:  tensor([925.5696], grad_fn=<AddBackward0>)\n",
      "L2:  0.01 , LR:  0.01 , Loss in Validation:  tensor([971.9980], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([16612.6211], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([992.4955], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([953.5855], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([938.4354], grad_fn=<AddBackward0>)\n",
      "L2:  0.01 , LR:  0.001 , Loss in Validation:  tensor([2010.6979], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([24957.1836], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([1030.1667], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([972.8727], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([951.3216], grad_fn=<AddBackward0>)\n",
      "L2:  0.01 , LR:  0.00075 , Loss in Validation:  tensor([2633.3635], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([19180030.], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Early stopping, Number of epochs:  102 , Loss in Validation:  tensor([1.1526e+18], grad_fn=<AddBackward0>) , Loss in Training:  tensor([548186.3750], grad_fn=<AddBackward0>)\n",
      "L2:  0.005 , LR:  0.03 , Loss in Validation:  tensor([1.1526e+18], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([84633.7500], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([926.4009], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Early stopping, Number of epochs:  326 , Loss in Validation:  tensor([992.5688], grad_fn=<AddBackward0>) , Loss in Training:  tensor([925.9813], grad_fn=<AddBackward0>)\n",
      "L2:  0.005 , LR:  0.01 , Loss in Validation:  tensor([992.5688], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([22613.1836], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([1016.2671], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([963.1703], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([944.2403], grad_fn=<AddBackward0>)\n",
      "L2:  0.005 , LR:  0.001 , Loss in Validation:  tensor([2361.6572], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([17274.3770], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([1020.8919], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([965.4774], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([947.5102], grad_fn=<AddBackward0>)\n",
      "L2:  0.005 , LR:  0.00075 , Loss in Validation:  tensor([2327.4268], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([1.2270e+12], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Early stopping, Number of epochs:  103 , Loss in Validation:  tensor([464787.9688], grad_fn=<AddBackward0>) , Loss in Training:  tensor([464635.3125], grad_fn=<AddBackward0>)\n",
      "L2:  0.001 , LR:  0.03 , Loss in Validation:  tensor([464787.9688], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([37811.8555], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([926.7455], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([925.5958], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "Loss in Train:  tensor([925.5829], grad_fn=<AddBackward0>)\n",
      "L2:  0.001 , LR:  0.01 , Loss in Validation:  tensor([971.9114], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([22191.7441], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([984.5304], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([946.5853], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Loss in Train:  tensor([935.2443], grad_fn=<AddBackward0>)\n",
      "L2:  0.001 , LR:  0.001 , Loss in Validation:  tensor([2308.8037], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([40407.2461], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([1084.5918], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([988.2703], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([962.6423], grad_fn=<AddBackward0>)\n",
      "L2:  0.001 , LR:  0.00075 , Loss in Validation:  tensor([3465.0889], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([16072.8213], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Loss in Train:  tensor([925.6606], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Early stopping, Number of epochs:  319 , Loss in Validation:  tensor([1021.1495], grad_fn=<AddBackward0>) , Loss in Training:  tensor([925.5955], grad_fn=<AddBackward0>)\n",
      "Optimal L2:  0.001 , Optimal LR:  0.01\n"
     ]
    }
   ],
   "source": [
    "z_dim = 10\n",
    "input_n = 929\n",
    "Initial_Learning_Rate = [0.03, 0.01, 0.001, 0.00075]\n",
    "L2_Lambda = [0.1, 0.01, 0.005, 0.001]\n",
    "patience = 100\n",
    "beta = 1000\n",
    "num_epochs = 600\n",
    "Num_EPOCHS = 2000\n",
    "x_train, ytime_train, yevent_train, age_train, cstage_train, hgrade_train, race_black_train, race_white_train = load_data(\"D:/DL/Variational autoencoder/Tryout_12_30_2020/divided_data/exp_20/data_tr_20.csv\", dtype)\n",
    "x_valid, ytime_valid, yevent_valid, age_valid, cstage_valid, hgrade_valid, race_black_valid, race_white_valid = load_data(\"D:/DL/Variational autoencoder/Tryout_12_30_2020/divided_data/exp_20/data_val_20.csv\", dtype)\n",
    "x_test, ytime_test, yevent_test, age_test, cstage_test, hgrade_test, race_black_test, race_white_test = load_data(\"D:/DL/Variational autoencoder/Tryout_12_30_2020/divided_data/exp_20/data_tes_20.csv\", dtype)\n",
    "opt_l2_loss = 0\n",
    "opt_lr_loss = 0\n",
    "opt_loss = torch.Tensor([float(\"Inf\")])\n",
    "if torch.cuda.is_available():\n",
    "    opt_loss = opt_loss.cuda()\n",
    "for l2 in L2_Lambda:\n",
    "    for lr in Initial_Learning_Rate:\n",
    "        loss_train, loss_valid, tr_mu, tr_logvar, val_mu, val_logvar = trainBetaVAE_H(x_train, x_valid, z_dim, input_n, lr, l2, num_epochs, patience, beta)\n",
    "        if loss_valid < opt_loss:\n",
    "            opt_l2_loss = l2\n",
    "            opt_lr_loss = lr\n",
    "            opt_loss = loss_valid\n",
    "        print (\"L2: \", l2, \", LR: \", lr, \", Loss in Validation: \", loss_valid)\n",
    "loss_train, loss_test, tr_mu, tr_logvar, tes_mu, tes_logvar = trainBetaVAE_H(x_train, x_test, z_dim, input_n, opt_lr_loss, opt_l2_loss, Num_EPOCHS, patience, beta)\n",
    "print (\"Optimal L2: \", opt_l2_loss, \", Optimal LR: \", opt_lr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([270, 10])\n"
     ]
    }
   ],
   "source": [
    "print(tr_mu.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_z = reparametrize(tr_mu, tr_logvar)\n",
    "tes_z = reparametrize(tes_mu, tes_logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([270, 10])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"D:/DL/Variational autoencoder/Tryout_12_30_2020/divided_data/exp_20/latent_features_20/beta_1000/tr_z_20.csv\", tr_z.cpu().detach().numpy(), delimiter = \",\")\n",
    "np.savetxt(\"D:/DL/Variational autoencoder/Tryout_12_30_2020/divided_data/exp_20/latent_features_20/beta_1000/tes_z_20.csv\", tes_z.cpu().detach().numpy(), delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tr_pre = torch.cat((tr_z, ytime_train, yevent_train, age_train, cstage_train, hgrade_train, race_black_train, race_white_train), 1)\n",
    "processed_tes_pre = torch.cat((tes_z, ytime_test, yevent_test, age_test, cstage_test, hgrade_test, race_black_test, race_white_test), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([270, 17])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tr_pre.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tr = pd.DataFrame(processed_tr_pre, columns = ['Z_1', 'Z_2', 'Z_3', 'Z_4', 'Z_5', 'Z_6', 'Z_7', \n",
    "                                                         'Z_8', 'Z_9', 'Z_10', 'OS.time', 'OS.event', 'age', \n",
    "                                                         'stageh', 'gradeh', 'race_black', 'race_white'])\n",
    "processed_tr = processed_tr.astype(float)\n",
    "processed_tes = pd.DataFrame(processed_tes_pre, columns = ['Z_1', 'Z_2', 'Z_3', 'Z_4', 'Z_5', 'Z_6', 'Z_7', \n",
    "                                                           'Z_8', 'Z_9', 'Z_10', 'OS.time', 'OS.event', 'age', \n",
    "                                                           'stageh', 'gradeh', 'race_black', 'race_white'])\n",
    "processed_tes = processed_tes.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Z_1       Z_2       Z_3       Z_4       Z_5       Z_6       Z_7  \\\n",
      "0  -0.300747 -0.045133 -1.574540 -0.750193  0.259187  0.877949  0.800760   \n",
      "1  -0.279796  1.967330  0.549393 -2.172440  0.156980 -1.666545  0.583835   \n",
      "2   0.506962  0.010425  1.180324  1.231991  0.608189 -0.098923 -1.325808   \n",
      "3  -0.530668  0.145328  0.318735  0.859650  0.946244  0.272163 -1.399646   \n",
      "4  -1.085055  1.625088  0.914776 -0.346729  0.324161 -0.889468  0.745793   \n",
      "..       ...       ...       ...       ...       ...       ...       ...   \n",
      "73 -0.046637 -0.075478  0.226700  1.838614  3.140055 -2.024279  0.820216   \n",
      "74  0.376160 -0.846681 -0.426726 -0.705482 -0.553578 -0.327714 -0.848431   \n",
      "75 -1.656599 -2.176720  0.085181  0.442308  1.331213 -0.056378  0.101221   \n",
      "76 -0.570199  0.292508  0.586788  1.595318  0.494548  1.103488  0.132523   \n",
      "77 -0.474622 -1.240323  1.478146 -0.680126 -0.407574 -0.792985 -0.481261   \n",
      "\n",
      "         Z_8       Z_9      Z_10  OS.time  OS.event   age  stageh  gradeh  \\\n",
      "0   1.598931  0.230828  0.095825   5481.0       0.0  59.0     1.0     1.0   \n",
      "1   0.087653  0.050393 -1.659469   4624.0       1.0  67.0     1.0     1.0   \n",
      "2  -3.465360  0.305236  0.249696   3819.0       1.0  66.0     1.0     1.0   \n",
      "3   1.041565 -0.384499  0.895977   3500.0       0.0  56.0     1.0     1.0   \n",
      "4  -0.756039 -0.362210 -0.557631   2661.0       0.0  66.0     1.0     1.0   \n",
      "..       ...       ...       ...      ...       ...   ...     ...     ...   \n",
      "73  0.023124 -0.406034  0.300424    138.0       1.0  63.0     1.0     1.0   \n",
      "74  0.608091 -0.523138 -0.096389    129.0       1.0  65.0     1.0     1.0   \n",
      "75  0.863087  0.680155  0.221448     61.0       1.0  66.0     1.0     1.0   \n",
      "76 -0.104051  0.314361  1.169460     44.0       0.0  52.0     1.0     1.0   \n",
      "77  0.244489  0.683045  0.093472      8.0       1.0  64.0     1.0     1.0   \n",
      "\n",
      "    race_black  race_white  \n",
      "0          0.0         1.0  \n",
      "1          0.0         1.0  \n",
      "2          0.0         1.0  \n",
      "3          0.0         1.0  \n",
      "4          0.0         1.0  \n",
      "..         ...         ...  \n",
      "73         0.0         1.0  \n",
      "74         0.0         1.0  \n",
      "75         0.0         1.0  \n",
      "76         0.0         1.0  \n",
      "77         0.0         1.0  \n",
      "\n",
      "[78 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "print(processed_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lifelines\n",
    "from lifelines import CoxPHFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tes['gradeh'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\lifelines\\utils\\__init__.py:1115: ConvergenceWarning: Column race_black have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['OS.event'].astype(bool)\n",
      ">>> print(df.loc[events, 'race_black'].var())\n",
      ">>> print(df.loc[~events, 'race_black'].var())\n",
      "\n",
      "A very low variance means that the column race_black completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "D:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\lifelines\\fitters\\coxph_fitter.py:999: ConvergenceWarning: Newton-Rhaphson convergence completed successfully but norm(delta) is still high, 0.159. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  utils.ConvergenceWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>lifelines.CoxPHFitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration col</th>\n",
       "      <td>'OS.time'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event col</th>\n",
       "      <td>'OS.event'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline estimation</th>\n",
       "      <td>breslow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of observations</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of events observed</th>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partial log-likelihood</th>\n",
       "      <td>-152.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time fit was run</th>\n",
       "      <td>2021-01-02 02:56:17 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>exp(coef)</th>\n",
       "      <th>se(coef)</th>\n",
       "      <th>coef lower 95%</th>\n",
       "      <th>coef upper 95%</th>\n",
       "      <th>exp(coef) lower 95%</th>\n",
       "      <th>exp(coef) upper 95%</th>\n",
       "      <th>z</th>\n",
       "      <th>p</th>\n",
       "      <th>-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Z_1</th>\n",
       "      <td>0.37</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2.13</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_2</th>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.97</td>\n",
       "      <td>-2.15</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_3</th>\n",
       "      <td>0.26</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_4</th>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.36</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_5</th>\n",
       "      <td>0.57</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.11</td>\n",
       "      <td>2.82</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_6</th>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.37</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_7</th>\n",
       "      <td>0.38</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.13</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_8</th>\n",
       "      <td>0.08</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_9</th>\n",
       "      <td>0.38</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_10</th>\n",
       "      <td>0.04</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stageh</th>\n",
       "      <td>1.41</td>\n",
       "      <td>4.08</td>\n",
       "      <td>1.12</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.46</td>\n",
       "      <td>36.45</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradeh</th>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-1.22</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.81</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_black</th>\n",
       "      <td>-18.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2907.34</td>\n",
       "      <td>-5716.41</td>\n",
       "      <td>5680.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_white</th>\n",
       "      <td>-3.24</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.45</td>\n",
       "      <td>-6.08</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-2.23</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Concordance</th>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Partial AIC</th>\n",
       "      <td>334.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-likelihood ratio test</th>\n",
       "      <td>25.87 on 15 df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-log2(p) of ll-ratio test</th>\n",
       "      <td>4.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cph = CoxPHFitter()\n",
    "cph.fit(processed_tes, duration_col='OS.time', event_col='OS.event')\n",
    "cph.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
