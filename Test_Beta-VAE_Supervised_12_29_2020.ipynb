{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from scipy.interpolate import interp1d\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data.sort_values(\"OS.time\",ascending = False, inplace = True)\n",
    "    x = data.drop([\"Patient_ID\", \"race_black\", \"race_white\", \"age\", \"stageh\",\"gradeh\", \"OS\", \"OS.time\"], axis = 1).values\n",
    "    ytime = data.loc[:, [\"OS.time\"]].values\n",
    "    yevent = data.loc[:, [\"OS\"]].values\n",
    "    age = data.loc[:, [\"age\"]].values\n",
    "    cstage = data.loc[:, [\"stageh\"]].values\n",
    "    hgrade = data.loc[:, [\"gradeh\"]].values\n",
    "    race_black = data.loc[:, [\"race_black\"]].values\n",
    "    race_white = data.loc[:, [\"race_white\"]].values\n",
    "    return(x, ytime, yevent, age, cstage, hgrade, race_black, race_white)\n",
    "\n",
    "def load_data(path, dtype):\n",
    "    x, ytime, yevent, age, cstage, hgrade, race_black, race_white = sort_data(path)\n",
    "    X = torch.from_numpy(x).type(dtype)\n",
    "    YTIME = torch.from_numpy(ytime).type(dtype)\n",
    "    YEVENT = torch.from_numpy(yevent).type(dtype)\n",
    "    AGE = torch.from_numpy(age).type(dtype)\n",
    "    CSTAGE = torch.from_numpy(cstage).type(dtype)\n",
    "    HGRADE = torch.from_numpy(hgrade).type(dtype)\n",
    "    RACE_BLACK = torch.from_numpy(race_black).type(dtype)\n",
    "    RACE_WHITE = torch.from_numpy(race_white).type(dtype)\n",
    "    if torch.cuda.is_available():\n",
    "        X = X.cuda()\n",
    "        YTIME = YTIME.cuda()\n",
    "        YEVENT = YEVENT.cuda()\n",
    "        AGE = AGE.cuda()\n",
    "        CSTAGE = CSTAGE.cuda()\n",
    "        HGRADE = HGRADE.cuda()\n",
    "        RACE_BLACK = RACE_BLACK.cuda()\n",
    "        RACE_WHITE = RACE_WHITE.cuda()\n",
    "    return(X, YTIME, YEVENT, AGE, CSTAGE, HGRADE, RACE_BLACK, RACE_WHITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience, verbose=False, delta=0):\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter % 20 == 0:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(x, x_recon):\n",
    "    batch_size = x.size(0)\n",
    "    assert batch_size != 0\n",
    "    \n",
    "    recon_loss = F.mse_loss(x_recon, x, size_average=False).div(batch_size)\n",
    "\n",
    "    return recon_loss\n",
    "\n",
    "def kl_divergence(mu, logvar):\n",
    "    batch_size = mu.size(0)\n",
    "    assert batch_size != 0\n",
    "    \n",
    "    klds = -0.5*(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    total_kld = klds.sum(1).mean(0, True)\n",
    "    dimension_wise_kld = klds.mean(0)\n",
    "    mean_kld = klds.mean(1).mean(0, True)\n",
    "\n",
    "    return total_kld, dimension_wise_kld, mean_kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_set(x):\n",
    "    n_sample = x.size(0)\n",
    "    matrix_ones = torch.ones(n_sample, n_sample)\n",
    "    indicator_matrix = torch.tril(matrix_ones)\n",
    "    return(indicator_matrix)\n",
    "\n",
    "def merged_loss_function(pred, ytime, yevent):\n",
    "    n_observed = yevent.sum(0)\n",
    "    ytime_indicator = R_set(ytime)\n",
    "    if torch.cuda.is_available():\n",
    "        ytime_indicator = ytime_indicator.cuda()\n",
    "    risk_set_sum = ytime_indicator.mm(torch.exp(pred)) \n",
    "    diff = pred - torch.log(risk_set_sum)\n",
    "    sum_diff_in_observed = torch.transpose(diff, 0, 1).mm(yevent)\n",
    "    cost = (- (sum_diff_in_observed / n_observed)).reshape((-1,))\n",
    "    return(cost)\n",
    "\n",
    "def c_index(pred, ytime, yevent):\n",
    "    n_sample = len(ytime)\n",
    "    ytime_indicator = R_set(ytime)\n",
    "    ytime_matrix = ytime_indicator - torch.diag(torch.diag(ytime_indicator))\n",
    "    censor_idx = (yevent == 0).nonzero()\n",
    "    zeros = torch.zeros(n_sample)\n",
    "    ytime_matrix[censor_idx, :] = zeros\n",
    "    pred_matrix = torch.zeros_like(ytime_matrix)\n",
    "    for j in range(n_sample):\n",
    "        for i in range(n_sample):\n",
    "            if pred[i] < pred[j]:\n",
    "                pred_matrix[j, i]  = 1\n",
    "            elif pred[i] == pred[j]: \n",
    "                pred_matrix[j, i] = 0.5\n",
    "    concord_matrix = pred_matrix.mul(ytime_matrix)\n",
    "    concord = torch.sum(concord_matrix)\n",
    "    epsilon = torch.sum(ytime_matrix)\n",
    "    concordance_index = torch.div(concord, epsilon)\n",
    "    if torch.cuda.is_available():\n",
    "        concordance_index = concordance_index.cuda()\n",
    "    return(concordance_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparametrize(mu, logvar):\n",
    "    std = logvar.div(2).exp()\n",
    "    eps = Variable(std.data.new(std.size()).normal_())\n",
    "    return mu + std*eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaVAE_H_sup(nn.Module):\n",
    "    \"\"\"Model proposed in original beta-VAE paper(Higgins et al, ICLR, 2017). Modifications made to best accommodate our data\"\"\"\n",
    "\n",
    "    def __init__(self, z_dim, input_n):\n",
    "        super(BetaVAE_H_sup, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.nc = input_n\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_n, 200),          \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(200, 50),         \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(50, z_dim*2)            \n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, 50),                             \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(50, 200),      \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(200, input_n)\n",
    "        )\n",
    "        self.clilayer = nn.Linear(z_dim + 5, 1, bias = False)\n",
    "        \n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            if block == \"clilayer\":\n",
    "                init.uniform_(self._modules[block].weight, -0.001, 0.001)\n",
    "            else:\n",
    "                for m in self._modules[block]:\n",
    "                    kaiming_init(m)\n",
    "\n",
    "    def forward(self, x, x_2, x_3, x_4, x_5, x_6):\n",
    "        distributions = self._encode(x)\n",
    "        mu = distributions[:, :self.z_dim]\n",
    "        logvar = distributions[:, self.z_dim:]\n",
    "        z = reparametrize(mu, logvar)\n",
    "        x_recon = self._decode(z)\n",
    "        x_cat = torch.cat((z, x_2, x_3, x_4, x_5, x_6), 1)\n",
    "        lin_pred = self.clilayer(x_cat)\n",
    "\n",
    "        return x_recon, mu, logvar, lin_pred\n",
    "\n",
    "    def _encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def _decode(self, z):\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBetaVAE_H_sup(train_x, train_age, train_cstage, train_hgrade, train_race_black, train_race_white, train_ytime, train_yevent,\n",
    "                       eval_x, eval_age, eval_cstage, eval_hgrade, eval_race_black, eval_race_white, eval_ytime, eval_yevent,\n",
    "                       z_dim, input_n, Learning_Rate, L2, Num_Epochs, patience, beta):\n",
    "    net = BetaVAE_H_sup(z_dim, input_n)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience = patience, verbose = False)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    opt = optim.Adam(net.parameters(), lr=Learning_Rate, weight_decay = L2)\n",
    "    for epoch in range(Num_Epochs+1):\n",
    "        net.train()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        x_recon, mu, logvar, lin_pred = net(train_x, train_age, train_cstage, train_hgrade, train_race_black, train_race_white)\n",
    "        recon_loss = reconstruction_loss(train_x, x_recon)\n",
    "        total_kld, dim_wise_kld, mean_kld = kl_divergence(mu, logvar)\n",
    "        cox_cost = merged_loss_function(lin_pred, train_ytime, train_yevent)\n",
    "        beta_vae_loss = recon_loss + beta*total_kld + cox_cost\n",
    "        \n",
    "        beta_vae_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        net.eval()\n",
    "        val_x_recon, val_mu, val_logvar, val_pred = net(eval_x, eval_age, eval_cstage, eval_hgrade, eval_race_black, eval_race_white)\n",
    "        val_recon_loss = reconstruction_loss(eval_x, val_x_recon)\n",
    "        val_total_kld, val_dim_wise_kld, val_mean_kld = kl_divergence(val_mu, val_logvar)\n",
    "        val_cox_cost = merged_loss_function(val_pred, eval_ytime, eval_yevent)\n",
    "        val_loss = val_recon_loss + beta*val_total_kld + val_cox_cost\n",
    "        \n",
    "        early_stopping(val_loss, net)\n",
    "        if early_stopping.early_stop:\n",
    "            net.train()\n",
    "            tr_x_recon, tr_mu, tr_logvar, tr_pred = net(train_x, train_age, train_cstage, train_hgrade, train_race_black, train_race_white)\n",
    "            tr_recon_loss = reconstruction_loss(train_x, tr_x_recon)\n",
    "            tr_total_kld, tr_dim_wise_kld, tr_mean_kld = kl_divergence(tr_mu, tr_logvar)\n",
    "            tr_cox_cost = merged_loss_function(tr_pred, train_ytime, train_yevent)\n",
    "            tr_loss = tr_recon_loss + beta*tr_total_kld + tr_cox_cost\n",
    "            \n",
    "            tr_cindex = c_index(tr_pred, train_ytime, train_yevent)\n",
    "            val_cindex = c_index(val_pred, eval_ytime, eval_yevent)\n",
    "            print(\"Early stopping, Number of epochs: \", epoch, \", Loss in Validation: \", val_loss, \", Loss in Training: \", tr_loss)\n",
    "            break\n",
    "        if epoch % 200 == 0:\n",
    "            net.train()\n",
    "            tr_x_recon, tr_mu, tr_logvar, tr_pred = net(train_x, train_age, train_cstage, train_hgrade, train_race_black, train_race_white)\n",
    "            tr_recon_loss = reconstruction_loss(train_x, tr_x_recon)\n",
    "            tr_total_kld, tr_dim_wise_kld, tr_mean_kld = kl_divergence(tr_mu, tr_logvar)\n",
    "            tr_cox_cost = merged_loss_function(tr_pred, train_ytime, train_yevent)\n",
    "            tr_loss = tr_recon_loss + beta*tr_total_kld + tr_cox_cost\n",
    "            \n",
    "            tr_cindex = c_index(tr_pred, train_ytime, train_yevent)\n",
    "            val_cindex = c_index(val_pred, eval_ytime, eval_yevent)\n",
    "            print(\"Loss in Train: \", tr_loss)\n",
    "    return (tr_loss, val_loss, tr_mu, tr_logvar, val_mu, val_logvar, tr_cindex, val_cindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in Train:  tensor([18426298.], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "L2:  0.1 , LR:  0.03 , Loss in Validation:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([26487.6426], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([931.1347], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Loss in Train:  tensor([930.4819], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Early stopping, Number of epochs:  454 , Loss in Validation:  tensor([880.3049], grad_fn=<AddBackward0>) , Loss in Training:  tensor([930.4682], grad_fn=<AddBackward0>)\n",
      "L2:  0.1 , LR:  0.01 , Loss in Validation:  tensor([880.3049], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([22501.1328], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([1007.5203], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([964.1816], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([946.6854], grad_fn=<AddBackward0>)\n",
      "L2:  0.1 , LR:  0.001 , Loss in Validation:  tensor([2080.5608], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([21802.3594], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([1014.8426], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([964.3949], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([949.8064], grad_fn=<AddBackward0>)\n",
      "L2:  0.1 , LR:  0.00075 , Loss in Validation:  tensor([2363.2632], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([1.5339e+12], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "L2:  0.01 , LR:  0.03 , Loss in Validation:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([16093.9619], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([930.5145], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "Loss in Train:  tensor([930.4468], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Early stopping, Number of epochs:  486 , Loss in Validation:  tensor([880.1332], grad_fn=<AddBackward0>) , Loss in Training:  tensor([930.4133], grad_fn=<AddBackward0>)\n",
      "L2:  0.01 , LR:  0.01 , Loss in Validation:  tensor([880.1332], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([18206.6660], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([996.5416], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([954.9439], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([940.5118], grad_fn=<AddBackward0>)\n",
      "L2:  0.01 , LR:  0.001 , Loss in Validation:  tensor([1755.0591], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([21930.2715], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([1041.8069], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([969.8436], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([952.1270], grad_fn=<AddBackward0>)\n",
      "L2:  0.01 , LR:  0.00075 , Loss in Validation:  tensor([2495.8167], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([12701154.], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "L2:  0.005 , LR:  0.03 , Loss in Validation:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([21552.5801], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([930.9196], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Early stopping, Number of epochs:  305 , Loss in Validation:  tensor([880.1284], grad_fn=<AddBackward0>) , Loss in Training:  tensor([930.5659], grad_fn=<AddBackward0>)\n",
      "L2:  0.005 , LR:  0.01 , Loss in Validation:  tensor([880.1284], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([15035.4453], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([1008.9957], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([961.6669], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Loss in Train:  tensor([944.7895], grad_fn=<AddBackward0>)\n",
      "L2:  0.005 , LR:  0.001 , Loss in Validation:  tensor([1438.9435], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([19802.3887], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([1019.8979], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([967.3747], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Loss in Train:  tensor([948.3674], grad_fn=<AddBackward0>)\n",
      "L2:  0.005 , LR:  0.00075 , Loss in Validation:  tensor([2571.9231], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([151251.0781], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "L2:  0.001 , LR:  0.03 , Loss in Validation:  tensor([nan], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([9199.4453], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([930.7075], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Early stopping, Number of epochs:  340 , Loss in Validation:  tensor([880.2202], grad_fn=<AddBackward0>) , Loss in Training:  tensor([930.5320], grad_fn=<AddBackward0>)\n",
      "L2:  0.001 , LR:  0.01 , Loss in Validation:  tensor([880.2202], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([25098.9570], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([978.6166], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([946.8619], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([937.1937], grad_fn=<AddBackward0>)\n",
      "L2:  0.001 , LR:  0.001 , Loss in Validation:  tensor([1901.3506], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([14236.5469], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([1012.8710], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([965.2715], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([945.6450], grad_fn=<AddBackward0>)\n",
      "L2:  0.001 , LR:  0.00075 , Loss in Validation:  tensor([2488.0286], grad_fn=<AddBackward0>)\n",
      "Loss in Train:  tensor([9629.2041], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Loss in Train:  tensor([930.6510], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Early stopping, Number of epochs:  313 , Loss in Validation:  tensor([976.8605], grad_fn=<AddBackward0>) , Loss in Training:  tensor([930.4092], grad_fn=<AddBackward0>)\n",
      "Optimal L2:  0.005 , Optimal LR:  0.01\n",
      "Training C-index:  tensor(0.6074) , testing C-index:  tensor(0.6378)\n"
     ]
    }
   ],
   "source": [
    "z_dim = 10\n",
    "input_n = 929\n",
    "Initial_Learning_Rate = [0.03, 0.01, 0.001, 0.00075]\n",
    "L2_Lambda = [0.1, 0.01, 0.005, 0.001]\n",
    "patience = 100\n",
    "beta = 1000\n",
    "num_epochs = 600\n",
    "Num_EPOCHS = 2000\n",
    "x_train, ytime_train, yevent_train, age_train, cstage_train, hgrade_train, race_black_train, race_white_train = load_data(\"D:/DL/Variational autoencoder/Tryout_12_22_2020/mir_train_normalized.csv\", dtype)\n",
    "x_valid, ytime_valid, yevent_valid, age_valid, cstage_valid, hgrade_valid, race_black_valid, race_white_valid = load_data(\"D:/DL/Variational autoencoder/Tryout_12_22_2020/mir_validation_normalized.csv\", dtype)\n",
    "x_test, ytime_test, yevent_test, age_test, cstage_test, hgrade_test, race_black_test, race_white_test = load_data(\"D:/DL/Variational autoencoder/Tryout_12_22_2020/mir_test_normalized.csv\", dtype)\n",
    "opt_l2_loss = 0\n",
    "opt_lr_loss = 0\n",
    "opt_loss = torch.Tensor([float(\"Inf\")])\n",
    "if torch.cuda.is_available():\n",
    "    opt_loss = opt_loss.cuda()\n",
    "for l2 in L2_Lambda:\n",
    "    for lr in Initial_Learning_Rate:\n",
    "        loss_train, loss_valid, tr_mu, tr_logvar, val_mu, val_logvar, tr_cindex, val_cindex = trainBetaVAE_H_sup(x_train, age_train, cstage_train, hgrade_train, race_black_train, race_white_train, ytime_train, yevent_train,\n",
    "                                                                                                             x_valid, age_valid, cstage_valid, hgrade_valid, race_black_valid, race_white_valid, ytime_valid, yevent_valid,\n",
    "                                                                                                             z_dim, input_n, lr, l2, num_epochs, patience, beta)\n",
    "        if loss_valid < opt_loss:\n",
    "            opt_l2_loss = l2\n",
    "            opt_lr_loss = lr\n",
    "            opt_loss = loss_valid\n",
    "        print (\"L2: \", l2, \", LR: \", lr, \", Loss in Validation: \", loss_valid)\n",
    "loss_train, loss_test, tr_mu, tr_logvar, tes_mu, tes_logvar, tr_cindex, tes_cindex = trainBetaVAE_H_sup(x_train, age_train, cstage_train, hgrade_train, race_black_train, race_white_train, ytime_train, yevent_train,\n",
    "                                                                                                    x_test, age_test, cstage_test, hgrade_test, race_black_test, race_white_test, ytime_test, yevent_test,\n",
    "                                                                                                    z_dim, input_n, opt_lr_loss, opt_l2_loss, Num_EPOCHS, patience, beta)\n",
    "print (\"Optimal L2: \", opt_l2_loss, \", Optimal LR: \", opt_lr_loss)\n",
    "print (\"Training C-index: \", tr_cindex, \", testing C-index: \", tes_cindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
