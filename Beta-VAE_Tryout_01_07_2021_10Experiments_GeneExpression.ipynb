{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from scipy.interpolate import interp1d\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data.sort_values(\"OS.time\",ascending = False, inplace = True)\n",
    "    x = data.drop([\"Patient_ID\", \"race_white\", \"age\", \"stageh\",\"gradeh\", \"OS\", \"OS.time\"], axis = 1).values\n",
    "    ytime = data.loc[:, [\"OS.time\"]].values\n",
    "    yevent = data.loc[:, [\"OS\"]].values\n",
    "    age = data.loc[:, [\"age\"]].values\n",
    "    cstage = data.loc[:, [\"stageh\"]].values\n",
    "    hgrade = data.loc[:, [\"gradeh\"]].values\n",
    "    race_white = data.loc[:, [\"race_white\"]].values\n",
    "    return(x, ytime, yevent, age, cstage, hgrade, race_white)\n",
    "\n",
    "def load_data(path, dtype):\n",
    "    x, ytime, yevent, age, cstage, hgrade, race_white = sort_data(path)\n",
    "    X = torch.from_numpy(x).type(dtype)\n",
    "    YTIME = torch.from_numpy(ytime).type(dtype)\n",
    "    YEVENT = torch.from_numpy(yevent).type(dtype)\n",
    "    AGE = torch.from_numpy(age).type(dtype)\n",
    "    CSTAGE = torch.from_numpy(cstage).type(dtype)\n",
    "    HGRADE = torch.from_numpy(hgrade).type(dtype)\n",
    "    RACE_WHITE = torch.from_numpy(race_white).type(dtype)\n",
    "    if torch.cuda.is_available():\n",
    "        X = X.cuda()\n",
    "        YTIME = YTIME.cuda()\n",
    "        YEVENT = YEVENT.cuda()\n",
    "        AGE = AGE.cuda()\n",
    "        CSTAGE = CSTAGE.cuda()\n",
    "        HGRADE = HGRADE.cuda()\n",
    "        RACE_WHITE = RACE_WHITE.cuda()\n",
    "    return(X, YTIME, YEVENT, AGE, CSTAGE, HGRADE, RACE_WHITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience, verbose=False, delta=0):\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter % 20 == 0:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(x, x_recon):\n",
    "    batch_size = x.size(0)\n",
    "    assert batch_size != 0\n",
    "    \n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction='sum').div(batch_size)\n",
    "\n",
    "    return recon_loss\n",
    "\n",
    "def kl_divergence(mu, logvar):\n",
    "    batch_size = mu.size(0)\n",
    "    assert batch_size != 0\n",
    "    \n",
    "    klds = -0.5*(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    total_kld = klds.sum(1).mean(0, True)\n",
    "    dimension_wise_kld = klds.mean(0)\n",
    "    mean_kld = klds.mean(1).mean(0, True)\n",
    "\n",
    "    return total_kld, dimension_wise_kld, mean_kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparametrize(mu, logvar):\n",
    "    std = logvar.div(2).exp()\n",
    "    eps = Variable(std.data.new(std.size()).normal_())\n",
    "    return mu + std*eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaVAE_H(nn.Module):\n",
    "    \"\"\"Model proposed in original beta-VAE paper(Higgins et al, ICLR, 2017). Modifications made to best accommodate our data\"\"\"\n",
    "\n",
    "    def __init__(self, z_dim, input_n):\n",
    "        super(BetaVAE_H, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.nc = input_n\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_n, 3200),          \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(3200, 800),          \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(800, 200),         \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(200, 50),         \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(50, z_dim*2)            \n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, 50),                             \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(50, 200),         \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(200, 800),         \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(800, 3200),      \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(3200, input_n)\n",
    "        )\n",
    "        \n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                kaiming_init(m)\n",
    "\n",
    "    def forward(self, x):\n",
    "        distributions = self._encode(x)\n",
    "        mu = distributions[:, :self.z_dim]\n",
    "        logvar = distributions[:, self.z_dim:]\n",
    "        z = reparametrize(mu, logvar)\n",
    "        x_recon = self._decode(z)\n",
    "\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "    def _encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def _decode(self, z):\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBetaVAE_H(train_x, eval_x, z_dim, input_n, Learning_Rate, L2, Num_Epochs, patience, beta):\n",
    "    net = BetaVAE_H(z_dim, input_n)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience = patience, verbose = False)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    opt = optim.Adam(net.parameters(), lr=Learning_Rate, weight_decay = L2)\n",
    "    for epoch in range(Num_Epochs+1):\n",
    "        net.train()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        x_recon, mu, logvar = net(train_x)\n",
    "        recon_loss = reconstruction_loss(train_x, x_recon)\n",
    "        total_kld, dim_wise_kld, mean_kld = kl_divergence(mu, logvar)\n",
    "        beta_vae_loss = recon_loss + beta*total_kld\n",
    "        \n",
    "        beta_vae_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        net.eval()\n",
    "        val_x_recon, val_mu, val_logvar = net(eval_x)\n",
    "        val_recon_loss = reconstruction_loss(eval_x, val_x_recon)\n",
    "        val_total_kld, val_dim_wise_kld, val_mean_kld = kl_divergence(val_mu, val_logvar)\n",
    "        val_loss = val_recon_loss + beta*val_total_kld\n",
    "        \n",
    "        early_stopping(val_loss, net)\n",
    "        if early_stopping.early_stop:\n",
    "            net.train()\n",
    "            tr_x_recon, tr_mu, tr_logvar = net(train_x)\n",
    "            tr_recon_loss = reconstruction_loss(train_x, tr_x_recon)\n",
    "            tr_total_kld, tr_dim_wise_kld, tr_mean_kld = kl_divergence(tr_mu, tr_logvar)\n",
    "            tr_loss = tr_recon_loss + beta*tr_total_kld\n",
    "            print(\"Early stopping, Number of epochs: \", epoch, \", Loss in Validation: \", val_loss, \", Loss in Training: \", tr_loss)\n",
    "            break\n",
    "        if epoch % 200 == 0:\n",
    "            net.train()\n",
    "            tr_x_recon, tr_mu, tr_logvar = net(train_x)\n",
    "            tr_recon_loss = reconstruction_loss(train_x, tr_x_recon)\n",
    "            tr_total_kld, tr_dim_wise_kld, tr_mean_kld = kl_divergence(tr_mu, tr_logvar)\n",
    "            tr_loss = tr_recon_loss + beta*tr_total_kld\n",
    "            print(\"Number of epochs: \", epoch, \", Loss in Train: \", tr_loss, \", Loss in validation: \", val_loss)\n",
    "    return (tr_loss, val_loss, tr_mu, tr_logvar, val_mu, val_logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs:  0 , Loss in Train:  tensor([9960807.], grad_fn=<AddBackward0>) , Loss in validation:  tensor([895640.2500], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Early stopping, Number of epochs:  103 , Loss in Validation:  tensor([3.6541e+10], grad_fn=<AddBackward0>) , Loss in Training:  tensor([1.8965e+24], grad_fn=<AddBackward0>)\n",
      "L2:  0.1 , Loss in Validation:  tensor([3.6541e+10], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  0 , Loss in Train:  tensor([5.1109e+09], grad_fn=<AddBackward0>) , Loss in validation:  tensor([8007647.], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "Number of epochs:  200 , Loss in Train:  tensor([12373.8701], grad_fn=<AddBackward0>) , Loss in validation:  tensor([12233.6816], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "Number of epochs:  400 , Loss in Train:  tensor([12071.5869], grad_fn=<AddBackward0>) , Loss in validation:  tensor([12142.1973], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Early stopping, Number of epochs:  484 , Loss in Validation:  tensor([12120.1104], grad_fn=<AddBackward0>) , Loss in Training:  tensor([12107.5576], grad_fn=<AddBackward0>)\n",
      "L2:  0.01 , Loss in Validation:  tensor([12120.1104], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  0 , Loss in Train:  tensor([599903.7500], grad_fn=<AddBackward0>) , Loss in validation:  tensor([140068.9375], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "Number of epochs:  200 , Loss in Train:  tensor([12202.3955], grad_fn=<AddBackward0>) , Loss in validation:  tensor([12186.3350], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Early stopping, Number of epochs:  235 , Loss in Validation:  tensor([12185.5586], grad_fn=<AddBackward0>) , Loss in Training:  tensor([12266.6689], grad_fn=<AddBackward0>)\n",
      "L2:  0.005 , Loss in Validation:  tensor([12185.5586], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  0 , Loss in Train:  tensor([6867453.5000], grad_fn=<AddBackward0>) , Loss in validation:  tensor([186461.0938], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Early stopping, Number of epochs:  198 , Loss in Validation:  tensor([12194.5703], grad_fn=<AddBackward0>) , Loss in Training:  tensor([12286.5244], grad_fn=<AddBackward0>)\n",
      "L2:  0.001 , Loss in Validation:  tensor([12194.5703], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  0 , Loss in Train:  tensor([1919040.7500], grad_fn=<AddBackward0>) , Loss in validation:  tensor([306612.0625], grad_fn=<AddBackward0>)\n",
      "Number of epochs:  200 , Loss in Train:  tensor([12307.2246], grad_fn=<AddBackward0>) , Loss in validation:  tensor([12328.2598], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Number of epochs:  400 , Loss in Train:  tensor([12218.2383], grad_fn=<AddBackward0>) , Loss in validation:  tensor([12243.0488], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "EarlyStopping counter: 20 out of 100\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Number of epochs:  600 , Loss in Train:  tensor([12119.3926], grad_fn=<AddBackward0>) , Loss in validation:  tensor([12158.9502], grad_fn=<AddBackward0>)\n",
      "EarlyStopping counter: 60 out of 100\n",
      "EarlyStopping counter: 80 out of 100\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Early stopping, Number of epochs:  660 , Loss in Validation:  tensor([12084.8965], grad_fn=<AddBackward0>) , Loss in Training:  tensor([12104.0088], grad_fn=<AddBackward0>)\n",
      "Optimal L2:  0.01 , Optimal Loss in Validation:  tensor([12120.1104], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z_dim = 25\n",
    "input_n = 12332\n",
    "lr = 0.001\n",
    "L2_Lambda = [0.1, 0.01, 0.005, 0.001]\n",
    "patience = 100\n",
    "beta = 125\n",
    "num_epochs = 600\n",
    "Num_EPOCHS = 2000\n",
    "x_train, ytime_train, yevent_train, age_train, cstage_train, hgrade_train, race_white_train = load_data(\"D:/DL/Variational autoencoder/Tryout_01_07_2021/divided_data/exp_1/data_tr_1.csv\", dtype)\n",
    "x_valid, ytime_valid, yevent_valid, age_valid, cstage_valid, hgrade_valid, race_white_valid = load_data(\"D:/DL/Variational autoencoder/Tryout_01_07_2021/divided_data/exp_1/data_val_1.csv\", dtype)\n",
    "x_test, ytime_test, yevent_test, age_test, cstage_test, hgrade_test, race_white_test = load_data(\"D:/DL/Variational autoencoder/Tryout_01_07_2021/divided_data/exp_1/data_tes_1.csv\", dtype)\n",
    "opt_l2_loss = 0\n",
    "opt_loss = torch.Tensor([float(\"Inf\")])\n",
    "if torch.cuda.is_available():\n",
    "    opt_loss = opt_loss.cuda()\n",
    "for l2 in L2_Lambda:\n",
    "    loss_train, loss_valid, tr_mu, tr_logvar, val_mu, val_logvar = trainBetaVAE_H(x_train, x_valid, z_dim, input_n, lr, l2, num_epochs, patience, beta)\n",
    "    if loss_valid < opt_loss:\n",
    "        opt_l2_loss = l2\n",
    "        opt_loss = loss_valid\n",
    "    print (\"L2: \", l2, \", Loss in Validation: \", loss_valid)\n",
    "loss_train, loss_test, tr_mu, tr_logvar, tes_mu, tes_logvar = trainBetaVAE_H(x_train, x_test, z_dim, input_n, lr, opt_l2_loss, Num_EPOCHS, patience, beta)\n",
    "print (\"Optimal L2: \", opt_l2_loss, \", Optimal Loss in Validation: \", opt_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([269, 25])\n"
     ]
    }
   ],
   "source": [
    "tr_z = reparametrize(tr_mu, tr_logvar)\n",
    "tes_z = reparametrize(tes_mu, tes_logvar)\n",
    "\n",
    "print(tr_z.size())\n",
    "\n",
    "#np.savetxt(\"D:/DL/Variational autoencoder/Tryout_01_07_2021/divided_data/exp_1/latent_features_1/beta_125/tr_z_1.csv\", tr_z.cpu().detach().numpy(), delimiter = \",\")\n",
    "#np.savetxt(\"D:/DL/Variational autoencoder/Tryout_01_07_2021/divided_data/exp_1/latent_features_1/beta_125/tes_z_1.csv\", tes_z.cpu().detach().numpy(), delimiter = \",\")\n",
    "\n",
    "processed_tr_pre = torch.cat((tr_z, ytime_train, yevent_train, age_train, cstage_train, hgrade_train, race_white_train), 1)\n",
    "processed_tes_pre = torch.cat((tes_z, ytime_test, yevent_test, age_test, cstage_test, hgrade_test, race_white_test), 1)\n",
    "\n",
    "processed_tr = pd.DataFrame(processed_tr_pre, columns = ['Z_1', 'Z_2', 'Z_3', 'Z_4', 'Z_5', 'Z_6', 'Z_7', \n",
    "                                                         'Z_8', 'Z_9', 'Z_10', 'Z_11', 'Z_12', 'Z_13', \n",
    "                                                         'Z_14', 'Z_15', 'Z_16', 'Z_17', 'Z_18', 'Z_19', \n",
    "                                                         'Z_20', 'Z_21', 'Z_22', 'Z_23', 'Z_24', 'Z_25', 'OS.time', 'OS.event', 'age', \n",
    "                                                         'stageh', 'gradeh', 'race_white'])\n",
    "processed_tr = processed_tr.astype(float)\n",
    "processed_tes = pd.DataFrame(processed_tes_pre, columns = ['Z_1', 'Z_2', 'Z_3', 'Z_4', 'Z_5', 'Z_6', 'Z_7', \n",
    "                                                           'Z_8', 'Z_9', 'Z_10', 'Z_11', 'Z_12', 'Z_13', \n",
    "                                                           'Z_14', 'Z_15', 'Z_16', 'Z_17', 'Z_18', 'Z_19', \n",
    "                                                           'Z_20', 'Z_21', 'Z_22', 'Z_23', 'Z_24', 'Z_25', 'OS.time', 'OS.event', 'age', \n",
    "                                                           'stageh', 'gradeh', 'race_white'])\n",
    "processed_tes = processed_tes.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Z_1       Z_2       Z_3       Z_4       Z_5       Z_6       Z_7  \\\n",
      "0   1.494316  0.781556 -1.147801  0.880775 -0.056339 -2.126174  1.255200   \n",
      "1   0.468572  1.142770  1.508999 -0.311027 -0.234807  0.120927 -0.466899   \n",
      "2   0.155375 -0.300734  0.066595 -1.085659 -0.875735  0.977980 -0.335594   \n",
      "3   1.442001  0.395555  1.502475 -1.119425  0.300102  0.869957 -0.616052   \n",
      "4  -0.735882  1.270305 -0.856642  0.305972  0.653906  0.635487 -0.833827   \n",
      "..       ...       ...       ...       ...       ...       ...       ...   \n",
      "90 -0.136664  1.290545  0.885622 -0.702092 -0.376044  0.941989  0.100371   \n",
      "91 -0.317133  0.984024 -0.199821 -0.189026 -0.229435 -0.934524 -0.305973   \n",
      "92 -0.751959 -0.076648 -2.176960 -0.503966 -0.923624 -0.376650  0.168631   \n",
      "93 -1.155667 -1.174169  0.043470  1.023346  0.166031  0.228310 -0.772525   \n",
      "94  1.546011  0.487233  1.392713  0.020101  0.512275 -0.986085  0.530113   \n",
      "\n",
      "         Z_8       Z_9      Z_10  ...      Z_22      Z_23      Z_24      Z_25  \\\n",
      "0  -0.556017 -0.488760 -1.412154  ... -0.344654  1.088518  0.259628  1.748817   \n",
      "1   0.128496  0.349444 -1.107189  ...  0.564099  0.429365 -0.480687  1.540961   \n",
      "2  -0.203016  1.652340  0.320411  ... -1.097591 -0.011394 -0.533637 -0.500414   \n",
      "3   0.678696 -0.043070 -0.550259  ... -0.218797  0.258768 -0.377165 -1.411154   \n",
      "4   0.299082 -1.271818 -1.992617  ...  1.917776  1.408724  0.620729 -1.085683   \n",
      "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
      "90  1.061804 -0.357055  0.508020  ... -0.756464 -0.685633 -0.931602  0.409262   \n",
      "91  1.357036 -1.044377 -0.943379  ...  0.267900 -0.836269  0.876146  0.263682   \n",
      "92  2.067937 -2.616052 -0.209229  ...  0.474620  1.335985 -1.793800  0.024312   \n",
      "93 -0.038626  1.018224 -0.907722  ... -1.534063 -0.583836  1.179192  0.814201   \n",
      "94  1.763431  0.409872 -0.191464  ...  0.507230  0.388244  0.309832 -0.015044   \n",
      "\n",
      "    OS.time  OS.event   age  stageh  gradeh  race_white  \n",
      "0    4624.0       1.0  67.0     1.0     1.0         1.0  \n",
      "1    3953.0       0.0  64.0     1.0     1.0         1.0  \n",
      "2    3785.0       1.0  62.0     1.0     1.0         1.0  \n",
      "3    2661.0       0.0  66.0     1.0     1.0         1.0  \n",
      "4    2648.0       1.0  76.0     1.0     0.0         1.0  \n",
      "..      ...       ...   ...     ...     ...         ...  \n",
      "90     31.0       1.0  75.0     1.0     1.0         1.0  \n",
      "91     25.0       1.0  70.0     1.0     1.0         1.0  \n",
      "92     24.0       1.0  76.0     1.0     1.0         1.0  \n",
      "93     16.0       0.0  63.0     0.0     1.0         1.0  \n",
      "94      8.0       1.0  64.0     1.0     1.0         1.0  \n",
      "\n",
      "[95 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print(processed_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lifelines\n",
    "from lifelines import CoxPHFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>lifelines.CoxPHFitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration col</th>\n",
       "      <td>'OS.time'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event col</th>\n",
       "      <td>'OS.event'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penalizer</th>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l1 ratio</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline estimation</th>\n",
       "      <td>breslow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of observations</th>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of events observed</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partial log-likelihood</th>\n",
       "      <td>-220.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time fit was run</th>\n",
       "      <td>2021-01-08 07:59:33 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>exp(coef)</th>\n",
       "      <th>se(coef)</th>\n",
       "      <th>coef lower 95%</th>\n",
       "      <th>coef upper 95%</th>\n",
       "      <th>exp(coef) lower 95%</th>\n",
       "      <th>exp(coef) upper 95%</th>\n",
       "      <th>z</th>\n",
       "      <th>p</th>\n",
       "      <th>-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Z_1</th>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.26</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_2</th>\n",
       "      <td>0.09</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_3</th>\n",
       "      <td>0.03</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_4</th>\n",
       "      <td>0.11</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_5</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_6</th>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.35</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_7</th>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.33</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_8</th>\n",
       "      <td>0.72</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.32</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.18</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>9.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_9</th>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.06</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_10</th>\n",
       "      <td>0.44</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_11</th>\n",
       "      <td>0.04</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_12</th>\n",
       "      <td>0.16</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_13</th>\n",
       "      <td>0.13</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_14</th>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.28</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_15</th>\n",
       "      <td>0.22</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.23</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_16</th>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-1.49</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_17</th>\n",
       "      <td>0.15</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_18</th>\n",
       "      <td>0.19</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_19</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_20</th>\n",
       "      <td>0.26</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_21</th>\n",
       "      <td>0.08</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_22</th>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.27</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_23</th>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.18</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_24</th>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_25</th>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.13</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.03</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stageh</th>\n",
       "      <td>1.57</td>\n",
       "      <td>4.79</td>\n",
       "      <td>0.92</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.79</td>\n",
       "      <td>29.19</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradeh</th>\n",
       "      <td>1.10</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.11</td>\n",
       "      <td>8.05</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_white</th>\n",
       "      <td>0.16</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.31</td>\n",
       "      <td>4.56</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Concordance</th>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Partial AIC</th>\n",
       "      <td>499.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-likelihood ratio test</th>\n",
       "      <td>47.99 on 29 df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-log2(p) of ll-ratio test</th>\n",
       "      <td>6.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cph = CoxPHFitter(l1_ratio = 1., penalizer = 0.0001)\n",
    "cph.fit(processed_tes, duration_col='OS.time', event_col='OS.event')\n",
    "cph.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"D:/DL/Variational autoencoder/Tryout_01_07_2021/divided_data/exp_1/latent_features_1/beta_125/tr_comb_z_1.csv\", tr_z.cpu().detach().numpy(), delimiter = \",\")\n",
    "np.savetxt(\"D:/DL/Variational autoencoder/Tryout_01_07_2021/divided_data/exp_1/latent_features_1/beta_125/tes_comb_z_1.csv\", tes_z.cpu().detach().numpy(), delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
